{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bLCf795lW-T"
      },
      "source": [
        "# Assignment 3\n",
        "\n",
        "## Instructions\n",
        "- Run this notebook on ```Google Colab(preferable)```\n",
        "- Write your code and analysis in the indicated cells.\n",
        "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
        "- Do not attempt to change the contents of other cells. \n",
        "\n",
        "## Packages Used\n",
        "- sklearn [link](https://scikit-learn.org/)\n",
        "- Keras [link](https://keras.io/guides/)\n",
        "\n",
        "## Submission\n",
        "- Rename the notebook to `<roll_number>_Assignment3_Q3.ipynb`.\n",
        "\n",
        "\n",
        "## Question 3\n",
        "Fake news is a widespread problem and there are many methods for combating it.\n",
        "You have to build a fake news detection system using a ML model. Train any ML model (ANN, LSTM) over the given Dataset.\n",
        "The dataset has short statements spoken by people and has the meta-information and corresponding label for those sentences. \n",
        "Your target is label column which has 6 labels(in the increasing order of truthfullness): pants-fire, false, barely-true, half-true, mostly-true, true.\n",
        "\n",
        "The features are 'statement', 'subject', 'speaker', 'job', 'state', 'party', 'barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c', 'pants_on_fire_c', 'venue' and the target is column \"label\".\n",
        "\n",
        "The statement is made by speaker whose job, party are given along with 6 columns which are an account of the  type of news(labels) the person has shared before. \n",
        "The person who has shared fake content before is likely to share it in future and this can be accounted by the ML model as a feature. Column barely_true_c contains how many barely_true news has the speaker shared (and so is with column X_c, value of X_c is number of X the person shared).\n",
        "\n",
        "\n",
        "You have to perform two tasks:\n",
        "* task1: Binary classification <br>\n",
        "Classify the given news as true/false. Take the labels pants-fire, false, barely-true as false and rest (half-true, mostly-true, true) as true.\n",
        "* task2: Six-way classification <br>\n",
        "Classify the given news into six-classes \"pants-fire, false, barely-true, half-true, mostly-true, true\".\n",
        "\n",
        "For each of the tasks:\n",
        "1) Experiment with depth of network and try to fine-tune hyperparameters reporting your observations. <br>\n",
        "2) Report the accuracy, f1-score, confusion matrix on train, val and test sets. <br>\n",
        "3) Experiment with bag-of-words, glove and bert embeddings(code given in the below notebook) and report results. <br> Comment on what is the affect of embedding on the results.\n",
        "\n",
        "The pre-processing code is provided, you need to write the training and test.\n",
        "\n",
        "Note: You are supposed to train on trainset, fine-tune on val and just eval on test set. If found that you trained on val/test sets, the penalty will be incurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCoLkHrylW-c"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy\n",
        "# !pip install tensorflow\n",
        "# !pip install re\n",
        "# !pip install nltk\n",
        "# !pip install keras\n",
        "# !pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agl6JEo_gaBT",
        "outputId": "1ea36171-9a8a-4921-9da1-dee89bc55219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras  #feel free to use any other library\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.utils import np_utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RB6GUgS8lW-i"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('q3_data/train.csv')\n",
        "val = pd.read_csv('q3_data/val.csv')\n",
        "test = pd.read_csv('q3_data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "enAZ4DvUffVr"
      },
      "outputs": [],
      "source": [
        "# Dropping the 'id' column\n",
        "train.drop('id', axis = 1, inplace = True)\n",
        "test.drop('id', axis = 1, inplace = True)\n",
        "val.drop('id', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "7pEJ-G4yITrd",
        "outputId": "68fcd3b1-4290-40fd-c70c-0739178397e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         label                                          statement  \\\n",
              "0        False  Says the Annies List political group supports ...   \n",
              "1    half-true  When did the decline of coal start? It started...   \n",
              "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
              "3        False  Health care reform legislation is likely to ma...   \n",
              "4    half-true  The economic turnaround started at the end of ...   \n",
              "\n",
              "                              subject         speaker                   job  \\\n",
              "0                            abortion    dwayne-bohac  State representative   \n",
              "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
              "2                      foreign-policy    barack-obama             President   \n",
              "3                         health-care    blog-posting                   NaN   \n",
              "4                        economy,jobs   charlie-crist                   NaN   \n",
              "\n",
              "      state       party  barely_true_c  false_c  half_true_c  mostly_true_c  \\\n",
              "0     Texas  republican              0        1            0              0   \n",
              "1  Virginia    democrat              0        0            1              1   \n",
              "2  Illinois    democrat             70       71          160            163   \n",
              "3       NaN        none              7       19            3              5   \n",
              "4   Florida    democrat             15        9           20             19   \n",
              "\n",
              "   pants_on_fire_c                venue  \n",
              "0                0             a mailer  \n",
              "1                0      a floor speech.  \n",
              "2                9               Denver  \n",
              "3               44       a news release  \n",
              "4                2  an interview on CNN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73e4119b-8a22-47f3-bfe5-0805ca11778d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>job</th>\n",
              "      <th>state</th>\n",
              "      <th>party</th>\n",
              "      <th>barely_true_c</th>\n",
              "      <th>false_c</th>\n",
              "      <th>half_true_c</th>\n",
              "      <th>mostly_true_c</th>\n",
              "      <th>pants_on_fire_c</th>\n",
              "      <th>venue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70</td>\n",
              "      <td>71</td>\n",
              "      <td>160</td>\n",
              "      <td>163</td>\n",
              "      <td>9</td>\n",
              "      <td>Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>a news release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>an interview on CNN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73e4119b-8a22-47f3-bfe5-0805ca11778d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73e4119b-8a22-47f3-bfe5-0805ca11778d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73e4119b-8a22-47f3-bfe5-0805ca11778d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbFqDO8_U6df",
        "outputId": "dd9689dc-2528-4929-afcd-a2693500f450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10269, 13)\n",
            "(1284, 13)\n",
            "(1283, 13)\n"
          ]
        }
      ],
      "source": [
        "# Checking the shape of data\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYP4_-X_lW-m"
      },
      "source": [
        "## Clean and pre-process data\n",
        "* Replace missing values\n",
        "* Remove numbers and special characters\n",
        "* Convert to upper-case\n",
        "\n",
        "We experiment with two types of processing, one directly appending the other attributes like subject, job, state, party to sentence and then applying bag of words on it.\n",
        "\n",
        "Other being encoding sentence with glove embeddings and passing just that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w7tTpAClApgJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dataPreprocessing(data):\n",
        "    '''Function for cleaning the dataset\n",
        "    '''\n",
        "    corpus = []\n",
        "    # Missing values\n",
        "    data[\"job\"].fillna(\"no-job\", inplace = True)\n",
        "    data[\"state\"].fillna(\"no-state\", inplace = True)\n",
        "\n",
        "    for x in range(data.shape[0]):\n",
        "        statement = re.sub('[^a-zA-Z]', ' ', data['statement'][x]) # Removing all numbers and special characters\n",
        "        statement = statement.lower() # Converting uppercase to lowercase\n",
        "        statement = statement.split()\n",
        "        \n",
        "        # you can experiment with any other stemmers\n",
        "        ps = PorterStemmer()\n",
        "        statement = [ps.stem(word) for word in statement if not word in set(stopwords.words('english'))] # Stemming the dataset and removing stopwords\n",
        "        statement = ' '.join(statement)\n",
        "        subject = data['subject'][x].replace(',', ' ')\n",
        "        speaker = data['speaker'][x]\n",
        "        job = data['job'][x].lower()\n",
        "        # job = job.replace(' ', '-')\n",
        "        state = data['state'][x].lower()\n",
        "        party = data['party'][x].lower()\n",
        "        corpus.append(statement + ' '  + subject + ' ' + job + ' ' + state + ' ' + party)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uy1ikPhJ9LoS"
      },
      "outputs": [],
      "source": [
        "x_train = dataPreprocessing(train)\n",
        "x_val = dataPreprocessing(val) \n",
        "x_test = dataPreprocessing(test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RkoHIZslW-p",
        "outputId": "54297de6-5e5c-4fd7-f997-672f4129461a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10269, 1284, 1283)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(x_train), len(x_val), len(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "D9SQ3YHLlW-q"
      },
      "outputs": [],
      "source": [
        "corpus = x_train + x_val + x_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHR_FHypumEI",
        "outputId": "6fa532f5-b60f-4378-8865-c9feafd0658d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12836"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {'False': 0, 'True': 1, 'barely-true': 2, 'half-true': 3, 'mostly-true': 4, 'pants-fire': 5}\n",
        "def encode(y):\n",
        "  y_final = []\n",
        "  for i in y:\n",
        "    tmp = np.zeros(6)\n",
        "    tmp[label_dict[i]] = 1\n",
        "    y_final.append(tmp)\n",
        "  return np.array(y_final)"
      ],
      "metadata": {
        "id": "Bre45xkt5hXd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = encode(train['label'])\n",
        "y_val = encode(val['label'])\n",
        "y_test = encode(test['label'])"
      ],
      "metadata": {
        "id": "KMhIm0BC3Aj1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_yjxgZ-lW-r"
      },
      "source": [
        "## Using bag-of-words embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sopw2zusZwn4"
      },
      "outputs": [],
      "source": [
        "# Converting the corpus into bag-of-words\n",
        "cv = CountVectorizer(max_features = 8000)\n",
        "X = cv.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnEujpuTlW-s",
        "outputId": "6ef63e1e-147d-403b-a79e-e8121f43a47f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o06bP9FJEaMU",
        "outputId": "274d831f-06d0-4b11-958d-8f00bfb3d202"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12836, 8000)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOcHliFBlW-t",
        "outputId": "9564966c-cbed-42dc-d0a2-11f25c74ff6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['label', 'statement', 'subject', 'speaker', 'job', 'state', 'party',\n",
              "       'barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c',\n",
              "       'pants_on_fire_c', 'venue'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RCqMgDpiLDhu"
      },
      "outputs": [],
      "source": [
        "# Selecting the columns 'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c'\n",
        "label_cols = ['barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c',\n",
        "       'pants_on_fire_c']\n",
        "x_train2 = train[label_cols]\n",
        "x_val2 = val[label_cols]\n",
        "x_test2 = test[label_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QglKXzA_w6DH"
      },
      "outputs": [],
      "source": [
        "# Stacking x_train and x_train2 horizontally\n",
        "x_train_bow = np.hstack((X[:len(x_train)], x_train2))\n",
        "x_val_bow = np.hstack((X[len(x_train):len(x_train)+len(x_val)], x_val2))\n",
        "x_test_bow = np.hstack((X[len(x_train)+len(x_val):], x_test2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3pskgViw99U",
        "outputId": "a4fdb222-5d0f-44ef-dbf9-25c9ae9a538c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10269, 8005)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "x_train_bow.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOsYNotSlW-v"
      },
      "source": [
        "## Use of Glove Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tts1vRNlW-w"
      },
      "source": [
        "download glove embeddings from 'https://nlp.stanford.edu/data/glove.6B.zip','glove.6B.zip'\n",
        "and place in your current working folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6TgQtxLlW-w"
      },
      "outputs": [],
      "source": [
        "#!unzip \"glove.6B.zip\" -d \"glove\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdi-ZY5nlW-x"
      },
      "outputs": [],
      "source": [
        "emmbed_dict = {}\n",
        "with open('glove/glove.6B.200d.txt','r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:],'float32')\n",
        "    emmbed_dict[word]=vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvRx03qClW-x"
      },
      "outputs": [],
      "source": [
        "emmbed_dict['oov'] = np.zeros(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhkSx-6zlW-y"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcbW-iSXlW-y",
        "outputId": "16904982-c4dc-499c-bfef-30da599147ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/avani/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def dataPreprocessing_glove(data):\n",
        "    corpus = []\n",
        "    # Missing values\n",
        "    data[\"job\"].fillna(\"no-job\", inplace = True)\n",
        "    data[\"state\"].fillna(\"no-state\", inplace = True)\n",
        "\n",
        "    for x in range(data.shape[0]):\n",
        "        statement = re.sub('[^a-zA-Z]', ' ', data['statement'][x]) # Removing all numbers and special characters\n",
        "        statement = statement.lower() # Converting uppercase to lowercase\n",
        "        statement = word_tokenize(statement)\n",
        "\n",
        "        embed_statement = []\n",
        "        for w in statement:\n",
        "            if w in emmbed_dict:\n",
        "                embed_statement.append(emmbed_dict[w])\n",
        "            else:\n",
        "                embed_statement.append(emmbed_dict['oov'])\n",
        "         \n",
        "        # bonus: Think how you can encode the below features(hint: look upon label encoding or training your own word2vec or any other embedding model)\n",
        "    \n",
        "#         subject = data['subject'][x].replace(',', ' ')\n",
        "#         speaker = data['speaker'][x]\n",
        "#         job = data['job'][x].lower()\n",
        "#         # job = job.replace(' ', '-')\n",
        "#         state = data['state'][x].lower()\n",
        "#         party = data['party'][x].lower()\n",
        "        corpus.append(embed_statement)\n",
        "    \n",
        "    return np.array(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb7ynSuAlW-z",
        "outputId": "2a32a4e8-fb6f-46bb-bb45-f49bb82c66c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/avani/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "x_train_glove = dataPreprocessing_glove(train)\n",
        "x_val_glove = dataPreprocessing_glove(val) \n",
        "x_test_glove = dataPreprocessing_glove(test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVi0ToIelW-z"
      },
      "outputs": [],
      "source": [
        "x_train_glove = np.hstack((x_train_glove.reshape(-1,1), x_train2))\n",
        "x_val_glove = np.hstack((x_val_glove.reshape(-1,1), x_val2))\n",
        "x_test_glove = np.hstack((x_test_glove.reshape(-1,1), x_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0h7LqjtlW-0"
      },
      "source": [
        "## Use of bert embeddings\n",
        "note: we used our pre-processed code for bow which has the attributed appended to end the end of sentence. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVJZoWOylW-0"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "x_train_bert = np.hstack((model.encode(x_train), x_train2))\n",
        "x_val_bert = np.hstack((model.encode(x_val), x_val2))\n",
        "x_test_bert = np.hstack((model.encode(x_test), x_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Jb5P56lW-0"
      },
      "source": [
        "Now use the above 3 types of embedded inputs(bow, glove, bert embeddings) for the 2 classification tasks and compare their outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmavEzWHrTC8"
      },
      "source": [
        "# Six-way classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wAhr39Aq41J"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yJwZwMXANg9_"
      },
      "outputs": [],
      "source": [
        "num_classes = 6\n",
        "# Preprocessing function for the labels\n",
        "def categorize(data):\n",
        "    y = data[\"label\"].tolist()\n",
        "\n",
        "    # Encoding the Dependent Variable\n",
        "    labelencoder_y = LabelEncoder()\n",
        "    y = labelencoder_y.fit_transform(y)\n",
        "\n",
        "    # Converting to binary class matrix\n",
        "    y = np_utils.to_categorical(y, num_classes)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XIKTUSM3MJ-u"
      },
      "outputs": [],
      "source": [
        "y_train_six_way = categorize(train)\n",
        "y_test_six_way = categorize(test)\n",
        "y_val_six_way = categorize(val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN1W6Cw5lW-2"
      },
      "source": [
        "Build a model and pass bow, glove and bert embedded inputs: x_train_bow, x_train_glove, x_train_bert(similarly validate for val and report results on test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EjIwiJhZ7LMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y-dusAUolnI"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lmVnqhL_lW-3"
      },
      "outputs": [],
      "source": [
        "## write your code here\n",
        "# Initialize hyperparameters\n",
        "# Create model\n",
        "# train\n",
        "# test\n",
        "# report accuracy, f1-score and confusion matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dropout\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "     layers.Dense(units=4005,activation='relu'),\n",
        "     layers.Dense(units=2000,activation='relu'),\n",
        "     layers.Dense(units=1000,activation='relu'),\n",
        "     layers.Dense(units=500, activation='relu'),\n",
        "     layers.Dense(units=200, activation='relu'),\n",
        "     layers.Dense(units=60, activation='relu'),\n",
        "     layers.Dense(units=6, activation='softmax'),\n",
        "     ]\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"mse\",\n",
        "    metrics=\"accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Words\n",
        "h_bow = model.fit(\n",
        "    x_train_bow, y_train_six_way,\n",
        "    validation_data=(x_val_bow, y_val_six_way),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwXPRzbN8P98",
        "outputId": "62fe58e2-4ede-40db-b9a5-91b2c01c3629"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "161/161 [==============================] - 11s 42ms/step - loss: 0.1316 - accuracy: 0.3158 - val_loss: 0.1208 - val_accuracy: 0.4213\n",
            "Epoch 2/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.1076 - accuracy: 0.4654 - val_loss: 0.1168 - val_accuracy: 0.4260\n",
            "Epoch 3/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0906 - accuracy: 0.5468 - val_loss: 0.1209 - val_accuracy: 0.3995\n",
            "Epoch 4/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0746 - accuracy: 0.6320 - val_loss: 0.1364 - val_accuracy: 0.3980\n",
            "Epoch 5/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.0643 - accuracy: 0.6753 - val_loss: 0.1372 - val_accuracy: 0.3964\n",
            "Epoch 6/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0574 - accuracy: 0.7152 - val_loss: 0.1732 - val_accuracy: 0.3879\n",
            "Epoch 7/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.0540 - accuracy: 0.7305 - val_loss: 0.1350 - val_accuracy: 0.4065\n",
            "Epoch 8/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.0491 - accuracy: 0.7580 - val_loss: 0.1491 - val_accuracy: 0.3871\n",
            "Epoch 9/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0466 - accuracy: 0.7705 - val_loss: 0.1497 - val_accuracy: 0.3980\n",
            "Epoch 10/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0444 - accuracy: 0.7835 - val_loss: 0.1616 - val_accuracy: 0.3559\n",
            "Epoch 11/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0427 - accuracy: 0.7901 - val_loss: 0.1564 - val_accuracy: 0.3731\n",
            "Epoch 12/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.0412 - accuracy: 0.7971 - val_loss: 0.1669 - val_accuracy: 0.3575\n",
            "Epoch 13/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0394 - accuracy: 0.8048 - val_loss: 0.1650 - val_accuracy: 0.3738\n",
            "Epoch 14/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0385 - accuracy: 0.8178 - val_loss: 0.1643 - val_accuracy: 0.3840\n",
            "Epoch 15/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0369 - accuracy: 0.8209 - val_loss: 0.1638 - val_accuracy: 0.3832\n",
            "Epoch 16/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.0373 - accuracy: 0.8184 - val_loss: 0.1611 - val_accuracy: 0.3816\n",
            "Epoch 17/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0358 - accuracy: 0.8285 - val_loss: 0.1640 - val_accuracy: 0.3894\n",
            "Epoch 18/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.0339 - accuracy: 0.8352 - val_loss: 0.1655 - val_accuracy: 0.3738\n",
            "Epoch 19/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0339 - accuracy: 0.8408 - val_loss: 0.1661 - val_accuracy: 0.3785\n",
            "Epoch 20/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0329 - accuracy: 0.8399 - val_loss: 0.1665 - val_accuracy: 0.3692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of bag of words:\", model.evaluate(x_test_bow, y_test_six_way, batch_size=64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpQn-QDWkqSc",
        "outputId": "5823d4fb-34f0-49d0-aa94-3317ef102d63"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 10ms/step - loss: 0.1683 - accuracy: 0.3640\n",
            "Accuracy of bag of words: [0.1683412790298462, 0.3639906346797943]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Glove\n",
        "h_glove = model.fit(\n",
        "    x_train_glove, y_train_six_way,\n",
        "    validation_data=(x_val_glove, y_val_six_way),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        ")\n",
        "\n",
        "#Bert Embedding\n",
        "h_bert = model.fit(\n",
        "    x_train_glove, y_train_six_way,\n",
        "    validation_data=(x_val_glove, y_val_six_way),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        ")"
      ],
      "metadata": {
        "id": "LmwLGmeqWqXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of Glove:\", model.evaluate(x_test_glove, y_test_six_way, batch_size=64))\n",
        "print()\n",
        "print(\"Accuracy of bert:\", model.evaluate(x_test_bert, y_test_six_way, batch_size=64))"
      ],
      "metadata": {
        "id": "OLwc6otMkvI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.math import confusion_matrix\n",
        "#Bag of words Confusion Matrix\n",
        "bg_words = confusion_matrix(np.argmax(y_test_six_way, axis=1), np.argmax(model.predict(x_test_bow), axis=1))\n",
        "print(bg_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EyIsUjDWMo4",
        "outputId": "b6ccfe2b-41ad-4d1d-a074-41ed7e00f19c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 6), dtype=int32, numpy=\n",
              "array([[104,  37,  30,  39,  16,  24],\n",
              "       [ 27,  64,  17,  56,  33,  14],\n",
              "       [ 36,  34,  66,  47,  17,  14],\n",
              "       [ 34,  59,  25, 120,  18,  11],\n",
              "       [ 23,  55,  21,  73,  70,   7],\n",
              "       [ 14,   5,  16,   9,   5,  43]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Glove\n",
        "glove = confusion_matrix(np.argmax(y_test_six_way, axis=1), np.argmax(model.predict(x_test_glove), axis=1))\n",
        "#Bert\n",
        "bert = confusion_matrix(np.argmax(y_test_six_way, axis=1), np.argmax(model.predict(x_test_bert), axis=1))"
      ],
      "metadata": {
        "id": "IubpgKHdckXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "#Bag of words\n",
        "bg_f1 = f1_score(np.argmax(y_test_six_way, axis=1), np.argmax(model.predict(x_test_bow), axis=1), average=None)"
      ],
      "metadata": {
        "id": "zNAjG4tLd_QS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Glove\n",
        "glove_f1 = f1_score(np.argmax(y_test_six_way, axis=1), np.argmax(model.predict(x_test_glove), axis=1), average=None)\n",
        "#Bert\n",
        "bert_f1 = f1_score(np.argmax(y_test_six_way, axis=1), np.argmax(model.predict(x_test_bert), axis=1), average=None)"
      ],
      "metadata": {
        "id": "Om01YhD4hN64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctoTOw2uIK1G"
      },
      "source": [
        "# Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJUrQ1SrEBa"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eA3wQH1JinNx"
      },
      "outputs": [],
      "source": [
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Mk-q1zwVF5KZ"
      },
      "outputs": [],
      "source": [
        "# Function for preprocessing labels\n",
        "def dataPreprocessingBinary(data):\n",
        "    y = data[\"label\"].tolist()\n",
        "\n",
        "    # Changing the 'half-true', 'mostly-true', barely-true', 'pants-fire' labels to True/False for Binary Classification\n",
        "    for x in range(len(y)):\n",
        "        if(y[x] == 'half-true'):\n",
        "            y[x] = 'True'\n",
        "        elif(y[x] == 'mostly-true'):\n",
        "            y[x] = 'True'\n",
        "        elif(y[x] == 'barely-true'):\n",
        "            y[x] = 'False'\n",
        "        elif(y[x] == 'pants-fire'):\n",
        "            y[x] = 'False'\n",
        "\n",
        "    # Converting the lables into binary class matrix\n",
        "    labelencoder_y = LabelEncoder()\n",
        "    y = labelencoder_y.fit_transform(y)\n",
        "    y = np_utils.to_categorical(y, num_classes)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "REu1ue0xbuqp"
      },
      "outputs": [],
      "source": [
        "y_train_binary = dataPreprocessingBinary(train)\n",
        "y_test_binary = dataPreprocessingBinary(test)\n",
        "y_val_binary = dataPreprocessingBinary(val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI4PIrgR01Sd"
      },
      "source": [
        "## Model\n",
        "Build a model and pass bow, glove and bert embedded inputs: x_train_bow, x_train_glove, x_train_bert(similarly validate for val and report results on test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-J0inPaQb-8Y"
      },
      "outputs": [],
      "source": [
        "## write your code here\n",
        "# Initialize hyperparameters\n",
        "# Create model\n",
        "# train\n",
        "# test\n",
        "# report accuracy, f1-score and confusion matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dropout\n",
        "\n",
        "model_binary = keras.Sequential(\n",
        "    [\n",
        "     layers.Dense(units=4005,activation='relu'),\n",
        "     layers.Dense(units=2000,activation='relu'),\n",
        "     layers.Dense(units=1000,activation='relu'),\n",
        "     layers.Dense(units=500, activation='relu'),\n",
        "     layers.Dense(units=200, activation='relu'),\n",
        "     layers.Dense(units=60, activation='relu'),\n",
        "     layers.Dense(units=2, activation='softmax'),\n",
        "     ]\n",
        ")\n",
        "model_binary.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"mse\",\n",
        "    metrics=\"accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Words\n",
        "print(\"\\tBag of Words\")\n",
        "h_bow_binary = model_binary.fit(\n",
        "    x_train_bow, y_train_binary,\n",
        "    validation_data=(x_val_bow, y_val_binary),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        ")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLlTocdx8r0_",
        "outputId": "ceaafce4-9172-424c-9d2c-e0534f8cbbe3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tBag of Words\n",
            "Epoch 1/20\n",
            "161/161 [==============================] - 8s 42ms/step - loss: 0.2201 - accuracy: 0.6747 - val_loss: 0.2026 - val_accuracy: 0.6449\n",
            "Epoch 2/20\n",
            "161/161 [==============================] - 7s 43ms/step - loss: 0.1660 - accuracy: 0.7557 - val_loss: 0.1945 - val_accuracy: 0.6986\n",
            "Epoch 3/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.1333 - accuracy: 0.8035 - val_loss: 0.1985 - val_accuracy: 0.7157\n",
            "Epoch 4/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.1021 - accuracy: 0.8513 - val_loss: 0.2272 - val_accuracy: 0.7040\n",
            "Epoch 5/20\n",
            "161/161 [==============================] - 6s 39ms/step - loss: 0.0862 - accuracy: 0.8779 - val_loss: 0.2210 - val_accuracy: 0.7150\n",
            "Epoch 6/20\n",
            "161/161 [==============================] - 6s 39ms/step - loss: 0.0751 - accuracy: 0.8970 - val_loss: 0.2428 - val_accuracy: 0.6838\n",
            "Epoch 7/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0666 - accuracy: 0.9069 - val_loss: 0.2534 - val_accuracy: 0.6939\n",
            "Epoch 8/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0615 - accuracy: 0.9194 - val_loss: 0.2573 - val_accuracy: 0.6955\n",
            "Epoch 9/20\n",
            "161/161 [==============================] - 6s 39ms/step - loss: 0.0586 - accuracy: 0.9211 - val_loss: 0.2826 - val_accuracy: 0.6620\n",
            "Epoch 10/20\n",
            "161/161 [==============================] - 7s 41ms/step - loss: 0.0536 - accuracy: 0.9263 - val_loss: 0.2740 - val_accuracy: 0.6908\n",
            "Epoch 11/20\n",
            "161/161 [==============================] - 6s 40ms/step - loss: 0.0528 - accuracy: 0.9281 - val_loss: 0.3190 - val_accuracy: 0.6721\n",
            "Epoch 12/20\n",
            "161/161 [==============================] - 6s 39ms/step - loss: 0.0511 - accuracy: 0.9333 - val_loss: 0.2826 - val_accuracy: 0.6729\n",
            "Epoch 13/20\n",
            "161/161 [==============================] - 7s 45ms/step - loss: 0.0459 - accuracy: 0.9389 - val_loss: 0.2815 - val_accuracy: 0.6916\n",
            "Epoch 14/20\n",
            "161/161 [==============================] - 7s 46ms/step - loss: 0.0481 - accuracy: 0.9355 - val_loss: 0.2773 - val_accuracy: 0.6682\n",
            "Epoch 15/20\n",
            "161/161 [==============================] - 7s 43ms/step - loss: 0.0451 - accuracy: 0.9403 - val_loss: 0.2570 - val_accuracy: 0.6924\n",
            "Epoch 16/20\n",
            "161/161 [==============================] - 7s 44ms/step - loss: 0.0450 - accuracy: 0.9408 - val_loss: 0.2686 - val_accuracy: 0.6885\n",
            "Epoch 17/20\n",
            "161/161 [==============================] - 7s 43ms/step - loss: 0.0467 - accuracy: 0.9420 - val_loss: 0.2644 - val_accuracy: 0.7033\n",
            "Epoch 18/20\n",
            "161/161 [==============================] - 7s 43ms/step - loss: 0.0480 - accuracy: 0.9403 - val_loss: 0.2967 - val_accuracy: 0.6729\n",
            "Epoch 19/20\n",
            "161/161 [==============================] - 7s 43ms/step - loss: 0.0456 - accuracy: 0.9421 - val_loss: 0.2890 - val_accuracy: 0.6838\n",
            "Epoch 20/20\n",
            "161/161 [==============================] - 7s 46ms/step - loss: 0.0394 - accuracy: 0.9508 - val_loss: 0.2603 - val_accuracy: 0.7040\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of bag of words in binary:\", model_binary.evaluate(x_test_bow, y_test_binary, batch_size=64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo0_FV6Lk7ds",
        "outputId": "bbb701e3-43d4-4143-f9cb-1f89a8644665"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 10ms/step - loss: 0.2691 - accuracy: 0.7054\n",
            "Accuracy of bag of words in binary: [0.2691079080104828, 0.705377995967865]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of words Confusion Matrix\n",
        "bg_words_binary = confusion_matrix(np.argmax(y_test_binary, axis=1), np.argmax(model_binary.predict(x_test_bow), axis=1))\n",
        "print(bg_words_binary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qICY3uwtedc1",
        "outputId": "bc747bf1-9604-4d37-b986-17acbae407d5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[384 172]\n",
            " [206 521]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Words F1 Score\n",
        "bg_f1_binary = f1_score(np.argmax(y_test_binary, axis=1), np.argmax(model_binary.predict(x_test_bow), axis=1),  average=None)\n",
        "print(bg_f1_binary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJGNK3bohfHv",
        "outputId": "f73b8f26-d47d-474f-cd2f-12e4ed92e1b3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.67015707 0.73380282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Glove\n",
        "print(\"\\tGlove\")\n",
        "h_glove_binary = model.fit(\n",
        "    x_train_glove, y_train_binary,\n",
        "    validation_data=(x_val_glove, y_val_binary),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        ")\n",
        "print()\n",
        "#Bert Embedding\n",
        "print(\"\\tBert Embedding\")\n",
        "h_bert_binary = model.fit(\n",
        "    x_train_glove, y_train_binary,\n",
        "    validation_data=(x_val_glove, y_val_binary),\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        ")"
      ],
      "metadata": {
        "id": "mmmzt3Y097Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Glove accuracy\n",
        "print(\"Accuracy of glove in binary:\", model_binary.evaluate(x_test_glove, y_test_binary, batch_size=64))\n",
        "#Bert accuracy\n",
        "print(\"Accuracy of bert in binary:\", model_binary.evaluate(x_test_bert, y_test_binary, batch_size=64))"
      ],
      "metadata": {
        "id": "oca8ZPx1lJPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Glove Confusion Matrix\n",
        "glove_binary = confusion_matrix(np.argmax(y_test_binary, axis=1), np.argmax(model_binary.predict(x_test_glove), axis=1))\n",
        "print(glove_binary)\n",
        "\n",
        "#Bert Confusion Matrix\n",
        "bert_binary = confusion_matrix(np.argmax(y_test_binary, axis=1), np.argmax(model_binary.predict(x_test_bert), axis=1))\n",
        "print(bert_binary)"
      ],
      "metadata": {
        "id": "RoflNrlwh7qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Words F1 Score\n",
        "glove_f1_binary = f1_score(np.argmax(y_test_binary, axis=1), np.argmax(model_binary.predict(x_test_glove), axis=1),  average=None)\n",
        "print(glove_f1_binary)\n",
        "\n",
        "#Bag of Words F1 Score\n",
        "bert_f1_binary = f1_score(np.argmax(y_test_binary, axis=1), np.argmax(model_binary.predict(x_test_bert), axis=1),  average=None)\n",
        "print(bert_f1_binary)"
      ],
      "metadata": {
        "id": "iAyjGcariMuz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "2019101109_Assignment3_Q3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}